{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Récupérer nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',') #we got that csv after running the Text Preprocessing file\n",
    "data.columns = ['tweet', 'class']\n",
    "\n",
    "categories = [\"not_sexist\", \"sexist\"]\n",
    "    #               2161           989\n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X , y ,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokeniser les phrases et les catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token = []\n",
    "y_token = []\n",
    "\n",
    "for phrase in X_train:\n",
    "    X_token.append(word_tokenize(phrase))    \n",
    "    \n",
    "for categorie in y_train:\n",
    "    y_token.append(categorie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Construire un corpus contenant des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(X_token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Entraîner le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Tester le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-e61f75574bfa>:5: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  phrases_similaires = modele.docvecs.most_similar(positive = [test_doc_vecteur])\n"
     ]
    }
   ],
   "source": [
    "def predire_categorie(phrase):\n",
    "    phrase = phrase.lower()\n",
    "    test_doc = word_tokenize(phrase)\n",
    "    test_doc_vecteur = modele.infer_vector(test_doc)\n",
    "    phrases_similaires = modele.docvecs.most_similar(positive = [test_doc_vecteur]) \n",
    "    phrases_similaires_categories = [y_token[phrases_similaires[i][0]] for i in range(len(phrases_similaires))]\n",
    "\n",
    "    nb_0 = phrases_similaires_categories.count(0)\n",
    "    nb_1 = phrases_similaires_categories.count(1)\n",
    "    \n",
    "    if(nb_0 <= nb_1):\n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "predictions = []    \n",
    "for phrase in X_test:\n",
    "    predictions.append(predire_categorie(phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6708994708994709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_sexist       0.75      0.78      0.76       643\n",
      "      sexist       0.48      0.43      0.46       302\n",
      "\n",
      "    accuracy                           0.67       945\n",
      "   macro avg       0.61      0.61      0.61       945\n",
      "weighted avg       0.66      0.67      0.67       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \", np.mean(predictions == y_test))\n",
    "metrics.confusion_matrix(y_test, predictions)\n",
    "print(metrics.classification_report(y_test, predictions,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 0.9993289113044739), ('est', 0.9992777109146118), ('du', 0.9992663264274597), ('qui', 0.9992426037788391), ('cette', 0.9992223978042603), ('sur', 0.9992102980613708), ('comme', 0.9992092251777649), ('dans', 0.999208927154541), ('aux', 0.9991835355758667), ('pour', 0.999182939529419)]\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',') #we got that csv after running the Text Preprocessing file\n",
    "data.columns = ['tweet', 'class']\n",
    "    \n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_token = []\n",
    "# iterate through each sentence in the file\n",
    "for i in X:\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    X_token.append(temp)\n",
    "    \n",
    "X_train, X_test, y_train , y_test = train_test_split(X_token , y ,test_size=0.3)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(X_train, min_count = 1, window = 5)\n",
    "print(model1.wv.most_similar('homme'))\n",
    "\n",
    "# Print results\n",
    "\n",
    "#print(\"Cosine similarity between 'femme' and 'championne' - CBOW : \",model1.wv.similarity('femme', 'championne'))\n",
    "#print(\"Cosine similarity between 'femmes' and 'hommes' - CBOW : \",model1.wv.similarity('femmes', 'hommes'))\n",
    "\n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(X_train, min_count = 1, window = 5, sg = 1)\n",
    "\n",
    "# Print results\n",
    "#print(\"Cosine similarity between 'femme' and 'championne' - Skip Gram : \",model2.wv.similarity('femme', 'championne'))\n",
    "#print(\"Cosine similarity between 'femmes' and 'hommes' - Skip Gram : \",model2.wv.similarity('femmes', 'hommes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06719302208547963\n",
      "0.09673344925526245\n",
      "[('partie', 0.9478771686553955), ('origine', 0.9476953744888306), ('cc', 0.9467566013336182), ('entendre', 0.9463518261909485), ('soiree', 0.9462419748306274), ('fevrier', 0.9457537531852722), ('belle', 0.9455102682113647), ('pense', 0.945289671421051), ('situation', 0.9451764822006226), ('3', 0.945033609867096)]\n"
     ]
    }
   ],
   "source": [
    "from pyemd import emd\n",
    "print(model1.wv.wmdistance('femmes',\"femme\"))\n",
    "print(model2.wv.wmdistance('La pomme est verte',\"La femme est belle\"))\n",
    "\n",
    "print(model1.wv.most_similar_cosmul(positive=['femme', 'championne'], negative=['homme']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST MODEL CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(X_train, min_count = 1, window = 5)\n",
    "\n",
    "my_dict = dict({})\n",
    "for idx, key in enumerate(model1.wv.key_to_index):\n",
    "    my_dict[key] = model1.wv[key]\n",
    "    # Or my_dict[key] = model.wv.get_vector(key)\n",
    "    # Or my_dict[key] = model.wv.word_vec(key, use_norm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9771\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "max_length = 16\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size=160000\n",
    "test_portion=.1\n",
    "print(len(my_dict))\n",
    "# 9696,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9771\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer embedding_1 weight shape (9696, 100) is not compatible with provided weight shape ().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dd5f2648edd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0membeddings_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m model = tf.keras.Sequential([\n\u001b[0m\u001b[0;32m      5\u001b[0m     tf.keras.layers.Embedding(9696, 100, input_length=max_length, \n\u001b[0;32m      6\u001b[0m                               weights=[embeddings_matrix], trainable=True),\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0mref_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1851\u001b[1;33m           raise ValueError(\n\u001b[0m\u001b[0;32m   1852\u001b[0m               \u001b[1;34mf'Layer {self.name} weight shape {ref_shape} '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m               \u001b[1;34m'is not compatible with provided weight '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer embedding_1 weight shape (9696, 100) is not compatible with provided weight shape ()."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embeddings_matrix = my_dict\n",
    "print(len(embeddings_matrix))\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(9696, 100, input_length=max_length, \n",
    "                              weights=[embeddings_matrix], trainable=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'e': 1, 'a': 2, 'i': 3, 's': 4, 'n': 5, 'r': 6, 'o': 7, 't': 8, 'u': 9, 'l': 10, 'm': 11, 'c': 12, 'p': 13, 'd': 14, 'f': 15, 'v': 16, 'b': 17, 'g': 18, 'h': 19, 'q': 20, 'x': 21, 'j': 22, 'y': 23, 'z': 24, '1': 25, 'k': 26, '2': 27, '0': 28, 'w': 29, '5': 30, '8': 31, '7': 32, '3': 33, '9': 34, '6': 35, '4': 36, '|': 37, '+': 38, '=': 39, 'E': 40, 'U': 41, 'R': 42}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(949086, 5410800)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = gensim.models.Word2Vec(\n",
    "            X,\n",
    "            vector_size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "print(model_w2v.wv.key_to_index)\n",
    "\n",
    "model_w2v.train(X_train, total_examples=len(X_train), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652    certains font bannir pr tweet pdt d autres ne ...\n",
      "1936    exclu chez cliquedimanche  kassovitz1 nous rac...\n",
      "1614    es femme appelle parce es dehors te dit de rep...\n",
      "1193    creatrice de balancetonporc sandra muller pour...\n",
      "1186    blog participatif  pour denoncer exposer sur t...\n",
      "                              ...                        \n",
      "2835    segolene royal obtient finalement poste de rei...\n",
      "2189    parce discrimination sexiste injustice tant en...\n",
      "2869    ce femmes passent leur temps commenter coupe p...\n",
      "477     liberte importuner catherine deneuve assume ma...\n",
      "2929    phonandroid un decollete pour article balancet...\n",
      "Name: tweet, Length: 2203, dtype: object\n",
      "[('abuser', 0.00017630505), ('surveillant', 0.00017184459), ('boulot', 0.00017065757), ('motive', 0.00017030934)]\n"
     ]
    }
   ],
   "source": [
    "#model_w2v.predict(X_test)\n",
    "print(X_train)\n",
    "t = model2.predict_output_word([\"segolene\",\"royal\",\"obtient\"],topn=4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - CBOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',') #we got that csv after running the Text Preprocessing file\n",
    "data.columns = ['tweet', 'class']\n",
    "    \n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_token = []\n",
    "# iterate through each sentence in the file\n",
    "for i in X:\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    X_token.append(temp)\n",
    "    \n",
    "X_train, X_test, y_train , y_test = train_test_split(X_token , y ,test_size=0.3)\n",
    "\n",
    "# Create CBOW model\n",
    "model_CBOW = gensim.models.Word2Vec(X_train, min_count = 1, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_list_similar_words_tweet(tweet):\n",
    "    similar_words = []\n",
    "    for i in range(len(tweet)):\n",
    "        sw = model_CBOW.wv.most_similar(i)\n",
    "        for j in sw:\n",
    "            similar_words.append(j[0])\n",
    "                \n",
    "    return list(set(similar_words))\n",
    "        \n",
    "\n",
    "def get_list_tweet_from_a_word(word):\n",
    "    for i in X_train:\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "    \n",
    "print(get_list_similar_words_tweet(X_train[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
