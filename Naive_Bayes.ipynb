{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVES BAYES - Utillisation de MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"not_sexist\", \"sexist\"]\n",
    "    #               2161           989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',')\n",
    "data.columns = ['tweet', 'class']\n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X , y ,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenisation avec scikit-learrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 8646)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Des occurences aux fréquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 8646)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Construction du model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tests sur des exemples simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'La femme doit être dans la cuisine' => not_sexist\n",
      "'La femme est belle' => sexist\n",
      "'Un homme' => not_sexist\n",
      "'Les hommes sont tous les mêmes' => not_sexist\n"
     ]
    }
   ],
   "source": [
    "tweet_test = ['La femme doit être dans la cuisine', \"La femme est belle\", \"Un homme\", \"Les hommes sont tous les mêmes\"]\n",
    "X_new_counts = count_vect.transform(tweet_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(tweet_test, predicted):\n",
    "     print('%r => %s' % (doc, categories[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      "[[634   2]\n",
      " [279  30]]\n",
      "Accuracy :  0.7026455026455026\n",
      "Balanced accuracy 0.5469713622763632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_sexist       0.69      1.00      0.82       636\n",
      "      sexist       0.94      0.10      0.18       309\n",
      "\n",
      "    accuracy                           0.70       945\n",
      "   macro avg       0.82      0.55      0.50       945\n",
      "weighted avg       0.77      0.70      0.61       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_new_counts =  count_vect.transform(X_test)\n",
    "X_test_new_tfidf = tfidf_transformer.transform(X_test_new_counts)\n",
    "predicted = clf.predict(X_test_new_tfidf)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(metrics.confusion_matrix(y_test, predicted))\n",
    "print(\"Accuracy : \", np.mean(predicted == y_test))\n",
    "print(\"Balanced accuracy\", metrics.balanced_accuracy_score(y_test, predicted))\n",
    "print(metrics.classification_report(y_test, predicted,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1523, 1: 1523})\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_resample(np.array(X_train).reshape(-1,1), y_train)\n",
    "print(Counter(y_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3046, 8646)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_ros.ravel())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3046, 8646)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"y'a que les femmes qui pleurent\" => sexist\n",
      "\"C'est un homme.\" => sexist\n",
      "\n",
      "Accuracy :  0.7421383647798742\n",
      "\n",
      "Matrice de confusion : \n",
      "[[425 211]\n",
      " [117 519]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_sexist       0.78      0.67      0.72       636\n",
      "      sexist       0.71      0.82      0.76       636\n",
      "\n",
      "    accuracy                           0.74      1272\n",
      "   macro avg       0.75      0.74      0.74      1272\n",
      "weighted avg       0.75      0.74      0.74      1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_ros)\n",
    "docs_new = [\"y'a que les femmes qui pleurent\", \"C'est un homme.\"] \n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, categories[category]))\n",
    "        \n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_ros.ravel(), y_ros)\n",
    "\n",
    "\n",
    "# resampling X, y\n",
    "X_ros_test, y_ros_test = ros.fit_resample(np.array(X_test).reshape(-1,1), y_test)# new class distribution \n",
    "docs_test = X_ros_test.ravel()\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print()\n",
    "print(\"Accuracy : \", np.mean(predicted == y_ros_test))\n",
    "print()\n",
    "print(\"Matrice de confusion : \")\n",
    "print(metrics.confusion_matrix(y_ros_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_ros_test, predicted,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 680, 1: 680})\n"
     ]
    }
   ],
   "source": [
    "# instantiating the random over sampler \n",
    "rus = RandomUnderSampler()\n",
    "# resampling X, y\n",
    "X_rus, y_rus = rus.fit_resample(np.array(X_train).reshape(-1,1), y_train)# new class distribution \n",
    "print(Counter(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 6426)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_rus.ravel())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 6426)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"y'a que les femmes qui pleurent\" => sexist\n",
      "\"C'est un homme.\" => sexist\n",
      "\n",
      "Accuracy :  0.7411003236245954\n",
      "\n",
      "Matrice de confusion : \n",
      "[[190 119]\n",
      " [ 41 268]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_sexist       0.82      0.61      0.70       309\n",
      "      sexist       0.69      0.87      0.77       309\n",
      "\n",
      "    accuracy                           0.74       618\n",
      "   macro avg       0.76      0.74      0.74       618\n",
      "weighted avg       0.76      0.74      0.74       618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_rus)\n",
    "docs_new = [\"y'a que les femmes qui pleurent\", \"C'est un homme.\"] \n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, categories[category]))\n",
    "        \n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_rus.ravel(), y_rus)\n",
    "\n",
    "\n",
    "# resampling X, y\n",
    "X_rus_test, y_rus_test = rus.fit_resample(np.array(X_test).reshape(-1,1), y_test)# new class distribution \n",
    "docs_test = X_rus_test.ravel()\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print()\n",
    "print(\"Accuracy : \", np.mean(predicted == y_rus_test))\n",
    "print()\n",
    "print(\"Matrice de confusion : \")\n",
    "print(metrics.confusion_matrix(y_rus_test, predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_rus_test, predicted,target_names=categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = RandomOverSampler(sampling_strategy=0.5)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled: Counter({0: 1523, 1: 761})\n"
     ]
    }
   ],
   "source": [
    "X_over, y_over = over.fit_resample(np.array(X_train).reshape(-1,1), y_train)\n",
    "print(f\"Oversampled: {Counter(y_over)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Random Sampling: Counter({0: 951, 1: 761})\n"
     ]
    }
   ],
   "source": [
    "# now to comine under sampling \n",
    "X_ros, y_ros = under.fit_resample(X_over, y_over)\n",
    "print(f\"Combined Random Sampling: {Counter(y_ros)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1712, 7165)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_ros.ravel())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1712, 7165)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"y'a que les femmes qui pleurent\" => not_sexist\n",
      "\"C'est un homme.\" => sexist\n",
      "Accuracy :  0.7389937106918238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not_sexist       0.72      0.77      0.75       636\n",
      "      sexist       0.76      0.70      0.73       636\n",
      "\n",
      "    accuracy                           0.74      1272\n",
      "   macro avg       0.74      0.74      0.74      1272\n",
      "weighted avg       0.74      0.74      0.74      1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_ros)\n",
    "docs_new = [\"y'a que les femmes qui pleurent\", \"C'est un homme.\"]\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, categories[category]))\n",
    "        \n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_ros.ravel(), y_ros)\n",
    "\n",
    "X_ros_test, y_ros_test = ros.fit_resample(np.array(X_test).reshape(-1,1), y_test)# new class distribution \n",
    "docs_test = X_ros_test.ravel()\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(\"Accuracy : \", np.mean(predicted == y_ros_test))\n",
    "metrics.confusion_matrix(y_ros_test, predicted)\n",
    "print(metrics.classification_report(y_ros_test, predicted,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES - Approche manuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',') #we got that csv after running the Text Preprocessing file\n",
    "data.columns = ['tweet', 'class']\n",
    "    \n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X , y ,test_size=0.3)\n",
    "\n",
    "\n",
    "categories = [\"NotSexist\", \"Sexist\"]\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "X_train_0 = []\n",
    "X_train_1 = []\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==0:\n",
    "        X_train_0.append(X_train[i])\n",
    "    else:\n",
    "        X_train_1.append(X_train[i])\n",
    "\n",
    "vec_train_0 = CountVectorizer()\n",
    "X_c0 = vec_train_0.fit_transform(X_train_0)\n",
    "tdm_0 = pd.DataFrame(X_c0.toarray(), columns=vec_train_0.get_feature_names())\n",
    "\n",
    "\n",
    "vec_train_1 = CountVectorizer()\n",
    "X_c1 = vec_train_1.fit_transform(X_train_1)\n",
    "tdm_1 = pd.DataFrame(X_c1.toarray(), columns=vec_train_1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_0 = vec_train_0.get_feature_names();    \n",
    "count_list_0 = X_c0.toarray().sum(axis=0) \n",
    "freq_0 = dict(zip(word_list_0,count_list_0))\n",
    "\n",
    "word_list_1 = vec_train_1.get_feature_names();    \n",
    "count_list_1 = X_c1.toarray().sum(axis=0) \n",
    "freq_1 = dict(zip(word_list_1,count_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = []\n",
    "for word,count in zip(word_list_0,count_list_0):\n",
    "    prob_0.append(count/len(word_list_0))\n",
    "    \n",
    "prob_1 = []\n",
    "for word,count in zip(word_list_1,count_list_1):\n",
    "    prob_1.append(count/len(word_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6533"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_0 = CountVectorizer()\n",
    "X_vec_0 = vec_0.fit_transform(X_train_0)\n",
    "\n",
    "total_features0 = len(vec_0.get_feature_names())\n",
    "total_features0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6194765787976484\n",
      "0.3805234212023516\n"
     ]
    }
   ],
   "source": [
    "vec_1 = CountVectorizer()\n",
    "X_vec_1 = vec_1.fit_transform(X_train_1)\n",
    "total_features1 = len(vec_1.get_feature_names())\n",
    "total_features1\n",
    "\n",
    "proba0 = total_features0 / (total_features0+total_features1)\n",
    "proba1 = total_features1 / (total_features0+total_features1)\n",
    "\n",
    "print(proba0)\n",
    "print(proba1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnts_features_0 = count_list_0.sum(axis=0)\n",
    "total_cnts_features_1 = count_list_1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def proba_sentence_class0(sentence, total_cnts_features, total_features):\n",
    "    new_word_list = word_tokenize(sentence)\n",
    "    prob_s_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_0.keys():\n",
    "            count = freq_0[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_s_with_ls.append((count + 1)/(total_cnts_features + total_features))\n",
    "    d = dict(zip(new_word_list,prob_s_with_ls))\n",
    "    res = 1\n",
    "    for word in sentence.split():\n",
    "        res = res * d[word]\n",
    "\n",
    "    res = res * proba0\n",
    "    return res\n",
    "\n",
    "def proba_sentence_class1(sentence,total_cnts_features, total_features):\n",
    "    new_word_list = word_tokenize(sentence)\n",
    "    prob_s_with_ls = []\n",
    "    for word in new_word_list:\n",
    "        if word in freq_1.keys():\n",
    "            count = freq_1[word]\n",
    "        else:\n",
    "            count = 0\n",
    "        prob_s_with_ls.append((count + 1)/(total_cnts_features + total_features))\n",
    "    d = dict(zip(new_word_list,prob_s_with_ls))\n",
    "    res = 1\n",
    "    for word in sentence.split():\n",
    "        res = res * d[word]\n",
    "\n",
    "    res = res * proba1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in X_test:\n",
    "    proba_class0 = proba_sentence_class0(i,total_cnts_features_0,total_features0)\n",
    "    proba_class1 = proba_sentence_class1(i,total_cnts_features_1,total_features1)\n",
    "    if proba_class0 > proba_class1:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6264550264550265\n",
      "\n",
      "Balanced accuracy : 0.7067441860465116\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NotSexist       0.93      0.49      0.64       645\n",
      "      Sexist       0.46      0.93      0.61       300\n",
      "\n",
      "    accuracy                           0.63       945\n",
      "   macro avg       0.70      0.71      0.63       945\n",
      "weighted avg       0.78      0.63      0.63       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_labels = len(y_test)\n",
    "acc = 0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==predictions[i]):\n",
    "        acc = acc + 1\n",
    "        \n",
    "acc = acc / total_labels\n",
    "print(\"Accuracy :\" , acc)\n",
    "print()\n",
    "print(\"Balanced accuracy :\" , metrics.balanced_accuracy_score(y_test, predictions))\n",
    "print()\n",
    "metrics.confusion_matrix(y_test, predictions)\n",
    "print(metrics.classification_report(y_test, predictions,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES - Approche manuelle + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',')\n",
    "data.columns = ['tweet', 'class']\n",
    "    \n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X , y ,test_size=0.3)\n",
    "\n",
    "\n",
    "categories = [\"NotSexist\", \"Sexist\"]\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "X_ros_train, y_ros_train = ros.fit_resample(np.array(X_train).reshape(-1,1), y_train)\n",
    "X_ros_test, y_ros_test = ros.fit_resample(np.array(X_test).reshape(-1,1), y_test)\n",
    "\n",
    "\n",
    "X_train_0 = []\n",
    "X_train_1 = []\n",
    "\n",
    "for i in range(len(y_ros_train)):\n",
    "    if y_ros_train[i]==0:\n",
    "        X_train_0.append(X_ros_train[i].tolist()[0])\n",
    "    else:\n",
    "        X_train_1.append(X_ros_train[i].tolist()[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vec_train_0 = CountVectorizer()\n",
    "X_c0 = vec_train_0.fit_transform(X_train_0)\n",
    "tdm_0 = pd.DataFrame(X_c0.toarray(), columns=vec_train_0.get_feature_names())\n",
    "\n",
    "\n",
    "vec_train_1 = CountVectorizer()\n",
    "X_c1 = vec_train_1.fit_transform(X_train_1)\n",
    "tdm_1 = pd.DataFrame(X_c1.toarray(), columns=vec_train_1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_0 = vec_train_0.get_feature_names();    \n",
    "count_list_0 = X_c0.toarray().sum(axis=0) \n",
    "freq_0 = dict(zip(word_list_0,count_list_0))\n",
    "\n",
    "word_list_1 = vec_train_1.get_feature_names();    \n",
    "count_list_1 = X_c1.toarray().sum(axis=0) \n",
    "freq_1 = dict(zip(word_list_1,count_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = []\n",
    "for word,count in zip(word_list_0,count_list_0):\n",
    "    prob_0.append(count/len(word_list_0))\n",
    "    \n",
    "prob_1 = []\n",
    "for word,count in zip(word_list_1,count_list_1):\n",
    "    prob_1.append(count/len(word_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6608"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_0 = CountVectorizer()\n",
    "X_vec_0 = vec_0.fit_transform(X_train_0)\n",
    "\n",
    "total_features0 = len(vec_0.get_feature_names())\n",
    "total_features0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6239848914069878\n",
      "0.3760151085930123\n"
     ]
    }
   ],
   "source": [
    "vec_1 = CountVectorizer()\n",
    "X_vec_1 = vec_1.fit_transform(X_train_1)\n",
    "total_features1 = len(vec_1.get_feature_names())\n",
    "total_features1\n",
    "\n",
    "proba0 = total_features0 / (total_features0+total_features1)\n",
    "proba1 = total_features1 / (total_features0+total_features1)\n",
    "\n",
    "print(proba0)\n",
    "print(proba1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnts_features_0 = count_list_0.sum(axis=0)\n",
    "total_cnts_features_1 = count_list_1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in X_ros_test:\n",
    "    proba_class0 = proba_sentence_class0(i[0],total_cnts_features_0,total_features0)\n",
    "    proba_class1 = proba_sentence_class1(i[0],total_cnts_features_1,total_features1)\n",
    "    if proba_class0 > proba_class1:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7589147286821706\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NotSexist       0.80      0.69      0.74       645\n",
      "      Sexist       0.73      0.82      0.77       645\n",
      "\n",
      "    accuracy                           0.76      1290\n",
      "   macro avg       0.76      0.76      0.76      1290\n",
      "weighted avg       0.76      0.76      0.76      1290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_labels = len(y_ros_test)\n",
    "acc = 0\n",
    "for i in range(len(y_ros_test)):\n",
    "    if(y_ros_test[i]==predictions[i]):\n",
    "        acc = acc + 1\n",
    "        \n",
    "acc = acc / total_labels\n",
    "print(\"Accuracy :\" , acc)\n",
    "print()\n",
    "metrics.confusion_matrix(y_ros_test, predictions)\n",
    "print(metrics.classification_report(y_ros_test, predictions,target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES - Approche manuelle + Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = pd.read_csv(\"my_csv_clean.csv\",sep = ',') #we got that csv after running the Text Preprocessing file\n",
    "data.columns = ['tweet', 'class']\n",
    "    \n",
    "\n",
    "X = data['tweet']\n",
    "y = data['class'] \n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X , y ,test_size=0.3)\n",
    "\n",
    "\n",
    "categories = [\"NotSexist\", \"Sexist\"]\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "X_rus_train, y_rus_train = rus.fit_resample(np.array(X_train).reshape(-1,1), y_train)# new class distribution \n",
    "X_rus_test, y_rus_test = rus.fit_resample(np.array(X_test).reshape(-1,1), y_test)# new class distribution \n",
    "\n",
    "\n",
    "X_train_0 = []\n",
    "X_train_1 = []\n",
    "\n",
    "for i in range(len(y_rus_train)):\n",
    "    if y_rus_train[i]==0:\n",
    "        X_train_0.append(X_rus_train[i].tolist()[0])\n",
    "    else:\n",
    "        X_train_1.append(X_rus_train[i].tolist()[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vec_train_0 = CountVectorizer()\n",
    "X_c0 = vec_train_0.fit_transform(X_train_0)\n",
    "tdm_0 = pd.DataFrame(X_c0.toarray(), columns=vec_train_0.get_feature_names())\n",
    "\n",
    "\n",
    "vec_train_1 = CountVectorizer()\n",
    "X_c1 = vec_train_1.fit_transform(X_train_1)\n",
    "tdm_1 = pd.DataFrame(X_c1.toarray(), columns=vec_train_1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_0 = vec_train_0.get_feature_names();    \n",
    "count_list_0 = X_c0.toarray().sum(axis=0) \n",
    "freq_0 = dict(zip(word_list_0,count_list_0))\n",
    "\n",
    "word_list_1 = vec_train_1.get_feature_names();    \n",
    "count_list_1 = X_c1.toarray().sum(axis=0) \n",
    "freq_1 = dict(zip(word_list_1,count_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = []\n",
    "for word,count in zip(word_list_0,count_list_0):\n",
    "    prob_0.append(count/len(word_list_0))\n",
    "    \n",
    "prob_1 = []\n",
    "for word,count in zip(word_list_1,count_list_1):\n",
    "    prob_1.append(count/len(word_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3799"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_0 = CountVectorizer()\n",
    "X_vec_0 = vec_0.fit_transform(X_train_0)\n",
    "\n",
    "total_features0 = len(vec_0.get_feature_names())\n",
    "total_features0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4894357124452461\n",
      "0.5105642875547539\n"
     ]
    }
   ],
   "source": [
    "vec_1 = CountVectorizer()\n",
    "X_vec_1 = vec_1.fit_transform(X_train_1)\n",
    "total_features1 = len(vec_1.get_feature_names())\n",
    "total_features1\n",
    "\n",
    "proba0 = total_features0 / (total_features0+total_features1)\n",
    "proba1 = total_features1 / (total_features0+total_features1)\n",
    "\n",
    "print(proba0)\n",
    "print(proba1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnts_features_0 = count_list_0.sum(axis=0)\n",
    "total_cnts_features_1 = count_list_1.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in X_rus_test:\n",
    "    proba_class0 = proba_sentence_class0(i[0],total_cnts_features_0,total_features0)\n",
    "    proba_class1 = proba_sentence_class1(i[0],total_cnts_features_1,total_features1)\n",
    "    if proba_class0 > proba_class1:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6889632107023411\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NotSexist       0.69      0.70      0.69       299\n",
      "      Sexist       0.69      0.68      0.69       299\n",
      "\n",
      "    accuracy                           0.69       598\n",
      "   macro avg       0.69      0.69      0.69       598\n",
      "weighted avg       0.69      0.69      0.69       598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_labels = len(y_rus_test)\n",
    "acc = 0\n",
    "for i in range(len(y_rus_test)):\n",
    "    if(y_rus_test[i]==predictions[i]):\n",
    "        acc = acc + 1\n",
    "        \n",
    "acc = acc / total_labels\n",
    "print(\"Accuracy :\" , acc)\n",
    "print()\n",
    "metrics.confusion_matrix(y_rus_test, predictions)\n",
    "print(metrics.classification_report(y_rus_test, predictions,target_names=categories))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
